{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Background","text":"<p>ERV identification, annotation and quantification pipeline built for ccbr1271. This pipeline detects and quantifies Endogenous Retroviruses using the scripts obtained from the Belkaid group.</p> <ul> <li>This pipeline built using Snakemake.</li> <li>This pipeline uses modules preinstalled and available on BIOWULF.</li> <li>This pipeline takes adapter removed and preprocess FASTQs from ccbr1271_longRNA pipeline as inputs.</li> <li>This pipeline has 3 distinct branches:</li> <li>Alignment with STAR followed by repeat elements quantification using HOMER</li> <li>Assembly of reads using MEGAHIT followed by contig alignments with NR database using DIAMOND</li> <li>Quantification + annotation of human ERVs using the hervQuant pipeline</li> </ul>"},{"location":"flowchart/","title":"Flowchart","text":"<p>DISCLAIMER: This is for v1.0.0. May not reflect any newer changes to the pipeline.</p>"},{"location":"resources/","title":"Resources","text":"<p>This pipeline requires a few resources to be created ahead of time to be supplied to the pipeline at runtime.</p>"},{"location":"resources/#genomes","title":"Genomes","text":"<p>mm10 (mouse) and hg38 (human) genomes were downloaded from UCSC directly. Only canonical chromosomes were retained, that is, chr1 through chr19 plus chrX, chrY and chrM for mouse; and chr1 through chr22 plus chrX, chrY and chrM for human.</p> <pre><code>\u2820\u2835 wget https://hgdownload.soe.ucsc.edu/goldenPath/mm10/bigZips/mm10.fa.gz\n\u2820\u2835 wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz\n\u2820\u2835 gzip -d hg38.fa.gz\n\u2820\u2835 mv hg38.fa hg38.full.fa\n\u2820\u2835 module load seqtk &amp;&amp; seqtk subseq hg38.full.fa chroms.lst &gt; hg38.fa\n\u2820\u2835 cat chroms.lst\nchr1\nchr2\nchr3\nchr4\nchr5\nchr6\nchr7\nchrX\nchr8\nchr9\nchr11\nchr10\nchr12\nchr13\nchr14\nchr15\nchr16\nchr17\nchr18\nchr20\nchr19\nchrY\nchr22\nchr21\nchrM\n</code></pre>"},{"location":"resources/#star-index","title":"STAR index","text":"<p>STAR indices were built using version 2.7.9a and located at <code>/data/EVset_RNAseq/Pipelines/ERVPipeline/resources</code>.</p>"},{"location":"resources/#geve-annotations","title":"GEVE annotations","text":"<ul> <li>GTF file is copied from <code>/data/EVset_RNAseq/chidesters2/ERV_code/Hsap38.geve.m_v2_SRK.gtf</code> and hardlinked as <code>hg38.geve.gtf</code></li> <li>GTF file is copied from <code>/data/EVset_RNAseq/chidesters2/ERV_code/Mmus38.geve.m_v2_SRK.gtf</code> and hardlinked as <code>mm10.geve.gtf</code></li> <li><code>mm10.geve.annotation_table.tsv</code></li> </ul> <pre><code>\u2820\u2835 wget //geve.med.u-tokai.ac.jp/download_data/table/Mmus38.txt.bz2\n\u2820\u2835 bzip2 -d Mmus38.txt.bz2\n\u2820\u2835 ln Mmus38.txt mm10.geve.annotation_table.tsv\n</code></pre> <ul> <li><code>hg38.geve.annotation_table.tsv</code></li> </ul> <pre><code>\u2820\u2835 wget http://geve.med.u-tokai.ac.jp/download_data/table/Hsap38.txt.bz2\n\u2820\u2835 bzip2 -d Hsap38.txt.bz2\n\u2820\u2835 ln Hsap38.txt hg38.geve.annotation_table.tsv\n</code></pre>"},{"location":"resources/#creating-taxid-to-lineage-lookup","title":"Creating taxid-to-lineage lookup","text":""},{"location":"resources/#download-dmp-files","title":"Download .dmp files","text":"<pre><code>\u2820\u2835 wget ftp://ftp.ncbi.nih.gov/pub/taxonomy/new_taxdump/new_taxdump.tar.gz  .\n\u2820\u2835 tar xzvf new_taxdump.tar.gz\n</code></pre> <p>The following <code>.dmp</code> files are extracted:</p> <ul> <li>citations.dmp</li> <li>delnodes.dmp</li> <li>division.dmp</li> <li>excludedfromtype.dmp</li> <li>fullnamelineage.dmp</li> <li>gencode.dmp</li> <li>host.dmp</li> <li>images.dmp</li> <li>merged.dmp</li> <li>names.dmp</li> <li>nodes.dmp</li> <li>rankedlineage.dmp</li> <li>taxidlineage.dmp</li> <li>typematerial.dmp</li> <li>typeoftype.dmp</li> </ul> <p>We only need <code>names.dmp</code> and <code>nodes.dmp</code></p>"},{"location":"resources/#ncbitax2lin","title":"ncbitax2lin","text":"<p>ncbitax2lin converts the <code>names.dmp</code> and <code>nodes.dmp</code> to a lineage csv file with columns:</p> <ul> <li>tax_id</li> <li>superkingdom</li> <li>phylum</li> <li>class</li> <li>order</li> <li>family</li> <li>genus</li> <li>species</li> <li>biotype</li> <li>clade</li> <li>clade1</li> <li>clade10</li> <li>clade11</li> <li>clade12</li> <li>clade13</li> <li>clade14</li> <li>clade15</li> <li>clade16</li> <li>clade17</li> <li>clade18</li> <li>clade19</li> <li>clade2</li> <li>clade3</li> <li>clade4</li> <li>clade5</li> <li>clade6</li> <li>clade7</li> <li>clade8</li> <li>clade9</li> <li>cohort</li> <li>forma</li> <li>forma specialis</li> <li>forma specialis1</li> <li>genotype</li> <li>infraclass</li> <li>infraorder</li> <li>isolate</li> <li>kingdom</li> <li>morph</li> <li>no rank</li> <li>no rank1</li> <li>no rank2</li> <li>no rank3</li> <li>no rank4</li> <li>no rank5</li> <li>parvorder</li> <li>pathogroup</li> <li>section</li> <li>series</li> <li>serogroup</li> <li>serotype</li> <li>species group</li> <li>species subgroup</li> <li>strain</li> <li>subclass</li> <li>subcohort</li> <li>subfamily</li> <li>subgenus</li> <li>subkingdom</li> <li>suborder</li> <li>subphylum</li> <li>subsection</li> <li>subspecies</li> <li>subtribe</li> <li>superclass</li> <li>superfamily</li> <li>superorder</li> <li>superphylum</li> <li>tribe</li> <li>varietas</li> </ul> <p>ncbitax2lin is available in the \"py311\" conda env and requires very large amount of memory. H ence, submitted like:</p> <pre><code>\u2820\u2835 sbatch slurm_job_high_memory ncbitaxid2lineage.sh\n</code></pre> <p>where</p> <pre><code>\u2820\u2835 more slurm_job_high_memory\n#!/bin/csh -v\n#SBATCH --job-name=\"taxid\"\n#SBATCH --mail-type=BEGIN,END\n#SBATCH --mem=1500g\n#SBATCH --partition=\"largemem\"\n#SBATCH --time=24:00:00\n\n\ncd $SLURM_SUBMIT_DIR\nsh $1\n</code></pre> <p>and</p> <pre><code>#!/bin/bash\n. \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\"\nconda activate py311\nncbitax2lin --nodes-file nodes.dmp --names-file names.dmp --output ncbi_lineages.csv.gz\n</code></pre>"},{"location":"resources/#diamond-index","title":"DIAMOND index","text":"<pre><code>\u2820\u2835 module load diamond\n\u2820\u2835 diamond makedb \\\n    --threads 56 \\\n    --in nr.gz \\\n    --db nr \\\n    --taxonmap prot.accession2taxid.FULL.gz \\\n    --taxonnodes nodes.dmp \\\n    --taxonnames names.dmp\n</code></pre> <ul> <li>creates <code>nr.dmnd</code></li> <li>it is about 300 GB in size</li> </ul>"},{"location":"resources/#creating-list-of-all-protein-accession-ids-in-nr","title":"Creating list of all protein accession ids in NR","text":""},{"location":"resources/#download-entire-nr","title":"Download entire nr","text":"<pre><code>\u2820\u2835 wget https://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nr.gz\n</code></pre>"},{"location":"resources/#extract-all-accession-ids-from-the-nrgz-file","title":"Extract all accession ids from the <code>nr.gz</code> file","text":"<pre><code>\u2820\u2835 zcat nr.gz|grep \"^&gt;\"|awk '{print $1}'|sed \"s/&gt;//g\" &gt; nr.accession_ids\n</code></pre> <p>This accounts to about 596 mil. accession ids. (~ 0.6 trillion)</p> <p>NOTE: some accession numbers have untrimmed prefixes or suffixes like: pir|E49255| prf||0803150A these should be E49255 0803150A</p>"},{"location":"resources/#split","title":"Split","text":"<pre><code>\u2820\u2835 split -d -a 5 -l 1000000 nr.accession_ids nr.accession_ids.\n</code></pre> <p>This generates ~600 files.</p>"},{"location":"resources/#get-the-accession_id-to-taxid-lookup","title":"Get the accession_id-to-taxid lookup","text":""},{"location":"resources/#download-it-and-split-it","title":"Download it, and split it","text":"<pre><code>\u2820\u2835 wget https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz\n\u2820\u2835 pigz -d -p 4 prot.accession2taxid.FULL.gz\n\u2820\u2835 split -d -a 5 -l 1000000 prot.accession2taxid.FULL prot.accession2taxid.\n</code></pre> <p>This creates 6356 files with 6355105289 total lines, i.e. ~ 6.4 trillion accession_id-to-taxid combinations.</p> <p>STATS:</p> <p>Total number of accession numbers in <code>nr.gz</code>: ~ 0.6 trillion</p> <p>Total number of protein accid to taxid rows in <code>prot.accession2taxid.FULL</code>: ~ 6.4 trillion</p>"},{"location":"resources/#add-taxid-information-from-protaccession2taxidfullgz-to-the-nr-accid-splits","title":"add taxid information from <code>prot.accession2taxid.FULL.gz</code> to the nr accid splits","text":"<pre><code>\u2820\u2835 for i in {00000..00595};do echo bash ./   add_taxid.sh nr.accession_ids.splits/nr.accession_ids.\n${i} nr.accession_ids.splits/nr.accession_ids.${i}.w_taxid;done &gt; do_it\n\n\u2820\u2835 swarm -f do_it -t 2 -g 120 --time=120:00:00\n</code></pre> <ul> <li>The <code>add_taxid.sh</code> is run for each nr.accession_ids.split. It iterates through each of the   6356 prot.accession2taxid splits and extracts the accid to taxid information. If taxid is not found the \"Unknown\" is reported.</li> <li>the <code>nr.accession_ids.?????.w_taxid</code> files contain a lot of \"Unknown\". (~ 13.5k per split)   . Some \"Unknown\" can be re-queried with ncbi using edirect utils and taxid information can be distilled</li> </ul>"},{"location":"resources/#update-unknowns","title":"update \"Unknowns\"","text":"<pre><code>\u2820\u2835 for f in {00000..00595};do echo \"bash ./post_process.sh $f &gt; ${f}.log 2&gt;&amp;1\";done &gt; do_update\n\n\u2820\u2835 swarm -f do_update -t 2 -m 120g --time=1:59:00 -m edirect --exclusive --maxrunning 1 --partition quick\n</code></pre> <ul> <li> <p><code>post_process.sh</code></p> </li> <li> <p>extracts \"Unknown\"s from the <code>*.w_taxid</code> files</p> </li> <li>splits the \"Unknown\"s into chunks of 100 each</li> <li>submit a query to NCBI using esummary with 100 accids at a time</li> <li>gather all new taxid information</li> <li> <p>save <code>*.w_taxid.updated</code> file for each nr.accession_ids split</p> </li> <li> <p>\"Unknown\"s now average to about 178 per nr.accession_ids split. There are a total of 106138 \"Unknowns\" in the entire ~0.6 trillion nr.accession_ids. Most of these are due to deprecated accession ids or proteins no longer being part of any NCBI genome. These can be safely ignored.</p> </li> </ul>"},{"location":"resources/#create-lookup-table-pickle","title":"create lookup table pickle","text":"<p>All the 596 updated files can be read into a single dict and saved as pickle for future 1-to-1 access. Memory needed to load the entire lookup table is around 130GB. The pkl file size is around 15GB.</p> <pre><code>\u2820\u2835 python topkl.py accid2taxid.pkl\n</code></pre> <ul> <li><code>accid2taxid.pkl</code> is 14GB in size</li> <li>loading the entire pkl in memory takes:</li> <li>~ 15 min</li> <li>~ 120 GB of RAM</li> </ul>"},{"location":"resources/#create-nr-titles-lookup-pickle","title":"Create nr titles lookup pickle","text":""},{"location":"resources/#extract-sequence-id-and-title","title":"extract sequence id and title","text":"<pre><code>\u2820\u2835 zcat nr.gz|grep \"^&gt;\"|sed \"s/&gt;//g\" &gt; nr.ids\n</code></pre>"},{"location":"resources/#build-pickle","title":"build pickle","text":"<pre><code>\u2820\u2835 python nr_topkl.py nr_titles.pkl\n</code></pre> <ul> <li><code>nr_titles.pkl</code> is ~ 72GB is size</li> <li>requires about 200 GB of RAM to load</li> </ul>"},{"location":"resources/#diamond-annotation-workflow","title":"DIAMOND annotation workflow","text":""},{"location":"usage/","title":"Usage","text":"<p>The code for this pipeline us located at <code>/data/EVset_RNAseq/Pipelines/ERVPipeline/&lt;version_number&gt;</code> on BIOWULF.</p> <pre><code>\u2820\u2835 ./erv\n\n##########################################################################################\n\nWelcome to\n\n\u256d\u2501\u2501\u2501\u256e\u2571\u2571\u2571\u2571\u256d\u256e\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u256d\u2501\u2501\u2501\u256e\u2571\u2571\u256d\u256e\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u256d\u2501\u2501\u2501\u256e\u2571\u2571\u2571\u2571\u2571\u2571\u256d\u256e\n\u2503\u256d\u2501\u2501\u256f\u2571\u2571\u2571\u2571\u2503\u2503\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2503\u256d\u2501\u256e\u2503\u2571\u256d\u256f\u2570\u256e\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2503\u256d\u2501\u256e\u2503\u2571\u2571\u2571\u2571\u2571\u2571\u2503\u2503\n\u2503\u2570\u2501\u2501\u2533\u2501\u256e\u256d\u2501\u256f\u2523\u2501\u2501\u2533\u2501\u2501\u2533\u2501\u2501\u2533\u2501\u256e\u256d\u2501\u2501\u2533\u2501\u2501\u2533\u256e\u256d\u2533\u2501\u2501\u256e\u2503\u2570\u2501\u256f\u2523\u2501\u253b\u256e\u256d\u254b\u2501\u2533\u2501\u2501\u2533\u256e\u256d\u2533\u2533\u2501\u2533\u256e\u256d\u2533\u2501\u2501\u256e\u2503\u2570\u2501\u256f\u2523\u2533\u2501\u2501\u2533\u2501\u2501\u252b\u2503\u256d\u2533\u2501\u256e\u256d\u2501\u2501\u256e\n\u2503\u256d\u2501\u2501\u252b\u256d\u256e\u252b\u256d\u256e\u2503\u256d\u256e\u2503\u256d\u256e\u2503\u2503\u2501\u252b\u256d\u256e\u252b\u2503\u2501\u252b\u256d\u256e\u2503\u2503\u2503\u2503\u2501\u2501\u252b\u2503\u256d\u256e\u256d\u252b\u2503\u2501\u252b\u2503\u2503\u256d\u252b\u256d\u256e\u2503\u2570\u256f\u2523\u252b\u256d\u252b\u2503\u2503\u2503\u2501\u2501\u252b\u2503\u256d\u2501\u2501\u254b\u252b\u256d\u256e\u2503\u2503\u2501\u252b\u2503\u2523\u252b\u256d\u256e\u252b\u2503\u2501\u252b\n\u2503\u2570\u2501\u2501\u252b\u2503\u2503\u2503\u2570\u256f\u2503\u2570\u256f\u2503\u2570\u256f\u2503\u2503\u2501\u252b\u2503\u2503\u2503\u2503\u2501\u252b\u2570\u256f\u2503\u2570\u256f\u2523\u2501\u2501\u2503\u2503\u2503\u2503\u2570\u252b\u2503\u2501\u252b\u2570\u252b\u2503\u2503\u2570\u256f\u2523\u256e\u256d\u252b\u2503\u2503\u2503\u2570\u256f\u2523\u2501\u2501\u2503\u2503\u2503\u2571\u2571\u2503\u2503\u2570\u256f\u2503\u2503\u2501\u252b\u2570\u252b\u2503\u2503\u2503\u2503\u2503\u2501\u252b\n\u2570\u2501\u2501\u2501\u253b\u256f\u2570\u253b\u2501\u2501\u253b\u2501\u2501\u253b\u2501\u256e\u2523\u2501\u2501\u253b\u256f\u2570\u253b\u2501\u2501\u253b\u2501\u2501\u253b\u2501\u2501\u253b\u2501\u2501\u256f\u2570\u256f\u2570\u2501\u253b\u2501\u2501\u253b\u2501\u253b\u256f\u2570\u2501\u2501\u256f\u2570\u256f\u2570\u253b\u256f\u2570\u2501\u2501\u253b\u2501\u2501\u256f\u2570\u256f\u2571\u2571\u2570\u252b\u256d\u2501\u253b\u2501\u2501\u253b\u2501\u253b\u253b\u256f\u2570\u253b\u2501\u2501\u256f\n\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u256d\u2501\u256f\u2503\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2503\u2503\n\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2570\u2501\u2501\u256f\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2571\u2570\u256f\n... v1.0.0\n\n##########################################################################################\n\nThis pipeline was built by CCBR (https://bioinformatics.ccr.cancer.gov/ccbr)\nPlease contact Vishal Koparde for comments/questions (vishal.koparde@nih.gov)\n\n##########################################################################################\n\nHere is a list of genome supported by this pipeline:\n\n  * hg38          [Human]\n  * mm10          [Mouse]\n\nUSAGE:\n  /path/to/erv -w/--workdir=&lt;WORKDIR&gt; -m/--runmode=&lt;RUNMODE&gt;\n\nRequired Arguments:\n1.  WORKDIR     : [Type: String]: Absolute or relative path to the output folder with write permissions.\n\n2.  RUNMODE     : [Type: String] Valid options:\n    * init      : initialize workdir\n    * dryrun    : dry run snakemake to generate DAG\n    * run       : run with slurm\n    * runlocal  : run without submitting to sbatch\n    ADVANCED RUNMODES (use with caution!!)\n    * unlock    : unlock WORKDIR if locked by snakemake NEVER UNLOCK WORKDIR WHERE PIPELINE IS CURRENTLY RUNNING!\n    * reconfig  : recreate config file in WORKDIR (debugging option) EDITS TO config.yaml WILL BE LOST!\n    * recopy    : recreate tools.yaml, cluster.yaml and scriptsdir in WORKDIR (debugging option) EDITS TO these files WILL BE LOST!\n    * reset     : DELETE workdir dir and re-init it (debugging option) EDITS TO ALL FILES IN WORKDIR WILL BE LOST!\n    * local     : same as runlocal\n\nOptional Arguments:\n\n--genome|-g     : genome eg. hg38(default) or mm10\n--manifest|-s   : absolute path to samples.tsv. This will be copied to output folder  (--runmode=init only)\n--help|-h       : print this help\n\nExample commands:\n  ./erv -w=/my/output/folder -m=init [ -g=\"mm10\" -s=\"/path/to/sample.tsv\" ]\n  ./erv -w=/my/output/folder -m=dryrun\n  ./erv -w=/my/output/folder -m=run\n\n##########################################################################################\n\nVersionInfo:\n  python          : 3.10\n  snakemake       : 7.32.3\n  pipeline_home   : /vf/users/EVset_RNAseq/Pipelines/ERVPipeline/dev\n  git commit/tag  : d74ba8f364c0f20d5cf175bb2568783c2abd8c56\n  pipeline_version: v1.0.0\n\n##########################################################################################\n</code></pre>"},{"location":"usage/#inputs","title":"Inputs","text":"<p>Samples are assumed to be single-end or paired-end fastqs which have already been adapter and UMI-trimmed. Intermediate fastqs from the longRNA pipeline can be used as inputs. The sample manifest is expected to be tab-delimited with these columns headers:</p> <ul> <li>replicateName</li> <li>sampleName</li> <li>stranded (Y or N)</li> <li>path_to_R1_fastq</li> <li>path_to_R2_fastq</li> </ul>"}]}